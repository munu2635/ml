{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 서포트 백터 머신 회귀(sklearn.svm.SVR)를 kernal=\"linear\"(하이퍼파라미터 c를 바꿔가며)나 kernal=\"rbf\"(하이퍼파라미터 C와 gamma를 바꿔가며)\n",
    "# 등의 다양한 하이퍼파라미터 설정으로 시도해보세요. 지금은 이 하이퍼파라미터가 무엇을 의미하는지 너무 신경쓰지마세요. 최상의 SVR모델은 무엇인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 추출하는 함수\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 테스트와 트레이닝 세트로 분리 \n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런을 이용하여 데이터셋을 여러 서브셋으로 만들 수 있다. \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "    \n",
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝을 위한 데이터 준비 //  예측 변수와 레이블을 분리 \n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# 데이터 정제 \n",
    "# total_bedrooms 특성에 값이 경우를 고치기 -- 어떤 값으로 채우기(fillna())\n",
    "median = housing[\"total_bedrooms\"].median()   # 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "\n",
    "# 사이킷런의 Imputer 를 이용하여 누락된 값을 손쉽게 다루도록 함\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy = \"median\") # 누락된 값을 특성의 중간값으로 변경 \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # 중간값이 수치형 특성에서만 계산될 수 있기때문에 텍스트 특성을 제외한 복사본 생성 \n",
    "imputer.fit(housing_num) # fit 메서드를 이용하여 훈련데이터에 적용\n",
    "\n",
    "# 어떤 수치형 데이터가 누락될지 모르니까 모든 수치형 특성에 imputer 특성을 적용하는것이 바람직 \n",
    "# imputer는 각 특성의 중간값을 계산하여 그 결과를 객체의 statistics 속성에 저장한다.\n",
    "# imputer.statistic_\n",
    "\n",
    "# 학습된 imputer 객체를 사용해 훈련세트에서 누락된 값을 학습한 중간값으로 바꿀수 있다. \n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        if self.categories != 'auto':\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                if self.handle_unknown == 'error':\n",
    "                    valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                    if not np.all(valid_mask):\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(self.categories[i])\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            valid_mask = np.in1d(Xi, self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    Xi = Xi.copy()\n",
    "                    Xi[~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        feature_indices = np.cumsum(n_values)\n",
    "\n",
    "        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n",
    "        indptr = X_mask.sum(axis=1).cumsum()\n",
    "        indptr = np.insert(indptr, 0, 0)\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csr_matrix((data, indices, indptr),\n",
    "                                shape=(n_samples, feature_indices[-1]),\n",
    "                                dtype=self.dtype)\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        n_features = len(self.categories_)\n",
    "        n_transformed_features = sum([len(cats) for cats in self.categories_])\n",
    "\n",
    "        # validate shape of passed X\n",
    "        msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n",
    "               \"columns, got {1}.\")\n",
    "        if self.encoding == 'ordinal' and X.shape[1] != n_features:\n",
    "            raise ValueError(msg.format(n_features, X.shape[1]))\n",
    "        elif (self.encoding.startswith('onehot')\n",
    "                and X.shape[1] != n_transformed_features):\n",
    "            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n",
    "\n",
    "        # create resulting array of appropriate dtype\n",
    "        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n",
    "        X_tr = np.empty((n_samples, n_features), dtype=dt)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            for i in range(n_features):\n",
    "                labels = X[:, i].astype('int64')\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "        else:  # encoding == 'onehot' / 'onehot-dense'\n",
    "            j = 0\n",
    "            found_unknown = {}\n",
    "\n",
    "            for i in range(n_features):\n",
    "                n_categories = len(self.categories_[i])\n",
    "                sub = X[:, j:j + n_categories]\n",
    "\n",
    "                # for sparse X argmax returns 2D matrix, ensure 1D array\n",
    "                labels = np.asarray(_argmax(sub, axis=1)).flatten()\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "                if self.handle_unknown == 'ignore':\n",
    "                    # ignored unknown categories: we have a row of all zero's\n",
    "                    unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n",
    "                    if unknown.any():\n",
    "                        found_unknown[i] = unknown\n",
    "\n",
    "                j += n_categories\n",
    "\n",
    "            # if ignored are found: potentially need to upcast result to\n",
    "            # insert None values\n",
    "            if found_unknown:\n",
    "                if X_tr.dtype != object:\n",
    "                    X_tr = X_tr.astype(object)\n",
    "\n",
    "                for idx, mask in found_unknown.items():\n",
    "                    X_tr[mask, idx] = None\n",
    "\n",
    "        return X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나만의 변환기 \n",
    "# 사이킷런의 기능과 매끄럽게 연동하고 싶을것입니다. 사이킷 런은 덕 타이핑을 지원하므로\n",
    "# fit()-(self를 반환), transfrom(), fit_transform()을 구현한 파이썬 클레스를 만들면 됨\n",
    "# 마지막 메소드(fit_transform())은 TransformerMixin을 상속하면 생성된다.\n",
    "# 또한 BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드를 (get_params(), set_params())를 얻게된다. \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix]/ X[:,household_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,bedrooms_ix]/ X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else :\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-121.89     37.29     38.     ...  710.      339.        2.7042]\n",
      " [-121.93     37.05     14.     ...  306.      113.        6.4214]\n",
      " [-117.2      32.77     31.     ...  936.      462.        2.8621]\n",
      " ...\n",
      " [-116.4      34.09      9.     ... 2098.      765.        3.2723]\n",
      " [-118.01     33.82     31.     ... 1356.      356.        4.0625]\n",
      " [-122.45     37.77     52.     ... 1269.      639.        3.575 ]]\n",
      "[['<1H OCEAN']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR OCEAN']\n",
      " ...\n",
      " ['INLAND']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR BAY']]\n"
     ]
    }
   ],
   "source": [
    "# 수치형 컬럼을 넘파이 배열로 추출하는대신 판다스의 데이터 프레임을 파이프라인에 직접 주입하면 좋다.\n",
    "# 수치형 컬럼을 넘파이 배열로 추출하는 대싱 판다스의 데이터 프레임을 다룰 수는 없지만 이를 처리하는 변환기를 만들수 있다. \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "class CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sparse_output=False):\n",
    "        self.sparse_output = sparse_output\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        enc = LabelBinarizer(sparse_output=self.sparse_output)\n",
    "        return enc.fit_transform(X)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 나머지는 버리고 필요한 특성을 선택하명 데이터 프레임을 넘파이 배열로바꾸는 식으로 데이터를 변환한다.\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X[self.attribute_names].values)\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)), # 수치형 특성을 선택한 걸로 파이프라인을 시작\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)), # 범주현 특성을 선택한걸로 파이프라인을 시작 \n",
    "        ('label_binarizer',CustomLabelBinarizer()),\n",
    "    ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion  # 파이프라인을 하나의 파이프라인으로 통합 \n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "housing_prepared = full_pipeline.fit_transform(housing) # 전체 파이프라인을 간단하게 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d79b312703b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrandomized_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_randomized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrandomized_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mcandidate_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# in this case we want to sample without replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         all_lists = np.all([not hasattr(v, \"rvs\")\n\u001b[0;32m--> 239\u001b[0;31m                             for v in self.param_distributions.values()])\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#  3x4=12, 2x3=6 => 18개 탐색 5번 모델을 훈련시킨다. 18x5=90 \n",
    "param_randomized = [ \n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "randomized_search = RandomizedSearchCV(forest_reg, param_randomized, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "randomized_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
