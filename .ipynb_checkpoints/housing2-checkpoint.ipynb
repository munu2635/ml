{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다운받는 함수 코드 \n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 추출하는 함수\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "#ocean_proximity만 제외하고 모든 특성이 숫자형이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 사용코드\n",
    "housing.head() # 데이터의 처음 5행 출력\n",
    "housing.info() # 데이터의 간략한 설명과 특히 전체 행 수, 각 특성의 데이터 타입과 널이 아닌 값의 개수를 확인하는데 유용 정보 출력\n",
    "housing[\"ocean_proximity\"].value_counts() # 범주형 특성의 정보 \n",
    "housing.describe() # 숫자형 특성의 요약정보 \n",
    "\n",
    "%matplotlib inline # 주피터 노트북의 매직 명령 주피터 자체의 백엔드를 사용 - 그래프는 노트북 안에 그려지게 됨 \n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트 만들기 \n",
    "# 샘플을 선택해 데이터 셋의 20퍼센트 정도 떼놓으면 된다. \n",
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런을 이용하여 데이터셋을 여러 서브셋으로 만들 수 있다. \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    0.350581\n",
       "2.0    0.318847\n",
       "4.0    0.176308\n",
       "5.0    0.114438\n",
       "1.0    0.039826\n",
       "Name: income_cat, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c94cf362c297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 지리적 데이터 시각화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhousing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scatter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# 밀집된 지역이 잘 부각된 산점도\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scatter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 주택 가격(=색)과 인구(=원의 반지름)가 포함된 산점도\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "# 지리적 데이터 시각화 \n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "# 밀집된 지역이 잘 부각된 산점도\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "# 주택 가격(=색)과 인구(=원의 반지름)가 포함된 산점도 \n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, \n",
    "             s=housing[\"population\"]/100, label=\"population\", figsize=(10, 7),\n",
    "             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True, sharex=False)\n",
    "plt.legend()\n",
    "# 표준상관관계 조사 corr()메서드를 이용\n",
    "corr_matrix = housing.corr()\n",
    "\n",
    "# 주택가격과 다른 특성 사이의 상관관계크기 살펴보기\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "# 산점도 행렬 \n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝을 위한 데이터 준비 \n",
    "# 예측 변수와 레이블을 분리 \n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 \n",
    "# total_bedrooms 특성에 값이 경우를 고치기\n",
    "# 방법은 3가지 - 해당구역 제거(dropna()), 전체 특성을 삭제(drop()), 어떤 값으로 채우기(fillna())\n",
    "housing.dropna(subset=[\"total_bedrooms\"])     # 1\n",
    "housing.drop(\"total_bedrooms\", axis=1)        # 2    \n",
    "median = housing[\"total_bedrooms\"].median()   # 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 Imputer 를 이용하여 누락된 값을 손쉽게 다루도록 함\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy = \"median\") # 누락된 값을 특성의 중간값으로 변경 \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # 중간값이 수치형 특성에서만 계산될 수 있기때문에 텍스트 특성을 제외한 복사본 생성 \n",
    "imputer.fit(housing_num) # fit 메서드를 이용하여 훈련데이터에 적용\n",
    "# 어떤 수치형 데이터가 누락될지 모르니까 모든 수치형 특성에 imputer 특성을 적용하는것이 바람직 \n",
    "# imputer는 각 특성의 중간값을 계산하여 그 결과를 객체의 statistics 속성에 저장한다.\n",
    "# imputer.statistic_\n",
    "\n",
    "# 학습된 imputer 객체를 사용해 훈련세트에서 누락된 값을 학습한 중간값으로 바꿀수 있다. \n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17606     <1H OCEAN\n",
       "18632     <1H OCEAN\n",
       "14650    NEAR OCEAN\n",
       "3230         INLAND\n",
       "3555      <1H OCEAN\n",
       "19480        INLAND\n",
       "8879      <1H OCEAN\n",
       "13685        INLAND\n",
       "4937      <1H OCEAN\n",
       "4861      <1H OCEAN\n",
       "Name: ocean_proximity, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트와 범주형 특성 다루기 \n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 0, 2, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대부분의 머신러닝 알고리즘은 숫자형을 다루므로 이 카테고리를 텍스트에서 숫자로 변경한다.\n",
    "# 각 카테고리를 다른 정숫값으로 매핑해주는 판다스의 factorize() 메서드를 사용\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['<1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카테고리 별 이진특성을 만들어 사용(카테고리가 <1H OCEAN 일때 한 특성은 1 나머지는 0으로 각각 카테고리별로 )\n",
    "# 원-핫 인코딩\n",
    "# 사이킷 런에서 숫자로된 범주형 값을 윈-핫 백터로 바꾸어주는 OneHotEncoder를 제공한다. \n",
    "\n",
    "from future_encoders import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(1, -1)) # 2차원 배열을 1차원 배열로 변경\n",
    "housing_cat_1hot\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        if self.categories != 'auto':\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                if self.handle_unknown == 'error':\n",
    "                    valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                    if not np.all(valid_mask):\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(self.categories[i])\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            valid_mask = np.in1d(Xi, self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    Xi = Xi.copy()\n",
    "                    Xi[~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        feature_indices = np.cumsum(n_values)\n",
    "\n",
    "        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n",
    "        indptr = X_mask.sum(axis=1).cumsum()\n",
    "        indptr = np.insert(indptr, 0, 0)\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csr_matrix((data, indices, indptr),\n",
    "                                shape=(n_samples, feature_indices[-1]),\n",
    "                                dtype=self.dtype)\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        n_features = len(self.categories_)\n",
    "        n_transformed_features = sum([len(cats) for cats in self.categories_])\n",
    "\n",
    "        # validate shape of passed X\n",
    "        msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n",
    "               \"columns, got {1}.\")\n",
    "        if self.encoding == 'ordinal' and X.shape[1] != n_features:\n",
    "            raise ValueError(msg.format(n_features, X.shape[1]))\n",
    "        elif (self.encoding.startswith('onehot')\n",
    "                and X.shape[1] != n_transformed_features):\n",
    "            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n",
    "\n",
    "        # create resulting array of appropriate dtype\n",
    "        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n",
    "        X_tr = np.empty((n_samples, n_features), dtype=dt)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            for i in range(n_features):\n",
    "                labels = X[:, i].astype('int64')\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "        else:  # encoding == 'onehot' / 'onehot-dense'\n",
    "            j = 0\n",
    "            found_unknown = {}\n",
    "\n",
    "            for i in range(n_features):\n",
    "                n_categories = len(self.categories_[i])\n",
    "                sub = X[:, j:j + n_categories]\n",
    "\n",
    "                # for sparse X argmax returns 2D matrix, ensure 1D array\n",
    "                labels = np.asarray(_argmax(sub, axis=1)).flatten()\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "                if self.handle_unknown == 'ignore':\n",
    "                    # ignored unknown categories: we have a row of all zero's\n",
    "                    unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n",
    "                    if unknown.any():\n",
    "                        found_unknown[i] = unknown\n",
    "\n",
    "                j += n_categories\n",
    "\n",
    "            # if ignored are found: potentially need to upcast result to\n",
    "            # insert None values\n",
    "            if found_unknown:\n",
    "                if X_tr.dtype != object:\n",
    "                    X_tr = X_tr.astype(object)\n",
    "\n",
    "                for idx, mask in found_unknown.items():\n",
    "                    X_tr[mask, idx] = None\n",
    "\n",
    "        return X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 카테고리를 숫자 카테고리로 숫자카테고리를 원-핫 벡터로 바꾸어주는 이 두가지 변환을 \n",
    "# CategoricalEncoder를 이용하여 한번에 할수 있다.\n",
    "\n",
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(1, -1)\n",
    "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "#cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나만의 변환기 \n",
    "# 사이킷런의 기능과 매끄럽게 연동하고 싶을것입니다. 사이킷 런은 덕 타이핑을 지원하므로\n",
    "# fit()-(self를 반환), transfrom(), fit_transform()을 구현한 파이썬 클레스를 만들면 됨\n",
    "# 마지막 메소드(fit_transform())은 TransformerMixin을 상속하면 생성된다.\n",
    "# 또한 BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드를 (get_params(), set_params())를 얻게된다. \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix]/ X[:,household_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,bedrooms_ix]/ X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else :\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 파이프라인 \n",
    "# 변환단계를 순서대로 실행될 수 있도록 사이킷런의 Pipeline 클레스를 이용하여 수정한다. \n",
    "\n",
    "# 연속된 단계를 나타내는 이름/추정기 쌍의 목록을 입력으로 받는다. \n",
    "# 마지막 단계에는 변환기와 추정기를 모두 사용할 수 있고 나머지는 모두 변환기이어야 한다. 즉 fit_transform() 메서드를 모두 갖고 있어야함\n",
    "# 마지막 추정기가 변환기 StandardScaler 이므로 파이프라인이 데이터에 대해 모든 변환을 순서대로 적용하는 transfrom() 메소드를 갖고있다. \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "using_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-121.89     37.29     38.     ...  710.      339.        2.7042]\n",
      " [-121.93     37.05     14.     ...  306.      113.        6.4214]\n",
      " [-117.2      32.77     31.     ...  936.      462.        2.8621]\n",
      " ...\n",
      " [-116.4      34.09      9.     ... 2098.      765.        3.2723]\n",
      " [-118.01     33.82     31.     ... 1356.      356.        4.0625]\n",
      " [-122.45     37.77     52.     ... 1269.      639.        3.575 ]]\n",
      "[['<1H OCEAN']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR OCEAN']\n",
      " ...\n",
      " ['INLAND']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR BAY']]\n"
     ]
    }
   ],
   "source": [
    "# 수치형 컬럼을 넘파이 배열로 추출하는대신 판다스의 데이터 프레임을 파이프라인에 직접 주입하면 좋다.\n",
    "# 수치형 컬럼을 넘파이 배열로 추출하는 대싱 판다스의 데이터 프레임을 다룰 수는 없지만 이를 처리하는 변환기를 만들수 있다. \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "class CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sparse_output=False):\n",
    "        self.sparse_output = sparse_output\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        enc = LabelBinarizer(sparse_output=self.sparse_output)\n",
    "        return enc.fit_transform(X)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 나머지는 버리고 필요한 특성을 선택하명 데이터 프레임을 넘파이 배열로바꾸는 식으로 데이터를 변환한다.\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X[self.attribute_names].values)\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)), # 수치형 특성을 선택한 걸로 파이프라인을 시작\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)), # 범주현 특성을 선택한걸로 파이프라인을 시작 \n",
    "        ('label_binarizer',CustomLabelBinarizer()),\n",
    "    ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion  # 파이프라인을 하나의 파이프라인으로 통합 \n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing) # 전체 파이프라인을 간단하게 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 세트에서 훈련하고 평가 하기\n",
    "# 완전히 작동하는 선형 회귀 모델\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68628.19819848922"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "#과소적합 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결정 트리 회귀 모델 \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "#과대적합 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차검증을 사용한 평가 (k-겹교차 검증) \n",
    "# 훈련세트를 폴드라고 불리는 10개의 서브셋으로 무작위 분할하여 10번 훈련후 평가 매번 다른 폴드를 선택해 평가에 사용 나머지 9개 훈련\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 결정 트리 결과\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [68881.36196262 67084.82309188 70231.72686057 69124.86032182\n",
      " 70554.05783656 74729.66421325 69456.37141762 70221.09347389\n",
      " 77169.7600642  70476.75722755]\n",
      "Mean:  70793.0476469977\n",
      "Standard deviation 2809.5160331056804\n"
     ]
    }
   ],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores: \" , scores)\n",
    "    print(\"Mean: \" , scores.mean())\n",
    "    print(\"Standard deviation\", scores.std())\n",
    "    \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "Mean:  69052.46136345083\n",
      "Standard deviation 2731.674001798348\n"
     ]
    }
   ],
   "source": [
    "# 선형 회귀 모델 \n",
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22024.67386238609"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomForestRegressor 모델 앙상블 학습 - 특성을 무작위로 선택해서 많은 결정트리를 만들고 그 예측을 평가하는 방법\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse)\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [52089.05595594 48701.14898599 51787.69210648 54723.34524624\n",
      " 52897.92645302 54992.16450494 51558.20042744 50978.77218868\n",
      " 56637.75233165 53024.03962986]\n",
      "Mean:  52739.00978302391\n",
      "Standard deviation 2155.1217054158305\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 세부 튜닝\n",
    "# 그리드 탐색 - 사이킷런 GridSearchCV 사용 RandomForestRegressor에 대한 최적의 하이퍼파라이터 조합을 탐색한다.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#  3x4=12, 2x3=6 => 18개 탐색 5번 모델을 훈련시킨다. 18x5=90 \n",
    "param_grid = [ \n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'n_estimators': 30}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적의 조합\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적의 추정기 직접 접근 \n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64305.9015998273 {'max_features': 2, 'n_estimators': 3}\n",
      "55610.781578271984 {'max_features': 2, 'n_estimators': 10}\n",
      "52885.7300030974 {'max_features': 2, 'n_estimators': 30}\n",
      "60880.47498323534 {'max_features': 4, 'n_estimators': 3}\n",
      "53310.87351152515 {'max_features': 4, 'n_estimators': 10}\n",
      "50530.03380608251 {'max_features': 4, 'n_estimators': 30}\n",
      "59066.4548258895 {'max_features': 6, 'n_estimators': 3}\n",
      "52159.77381362492 {'max_features': 6, 'n_estimators': 10}\n",
      "50067.65000471752 {'max_features': 6, 'n_estimators': 30}\n",
      "59031.04232971704 {'max_features': 8, 'n_estimators': 3}\n",
      "52103.51486569085 {'max_features': 8, 'n_estimators': 10}\n",
      "50213.82716088023 {'max_features': 8, 'n_estimators': 30}\n",
      "62198.285415524195 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54613.23216201322 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59798.09576363317 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52283.29378187078 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "59026.47222610855 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51943.54090295635 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# 평가 점수 확인하기\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.99809182e-02, 7.08073829e-02, 4.34923463e-02, 1.73935034e-02,\n",
       "       1.71711841e-02, 1.78652661e-02, 1.58548871e-02, 3.25076120e-01,\n",
       "       5.91630629e-02, 1.06543400e-01, 8.86884398e-02, 1.22462502e-02,\n",
       "       1.34065681e-01, 6.24104269e-05, 5.61723441e-03, 5.97191419e-03])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최상의 모델과 오차분석\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "feature_importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'classes_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b35052d3f987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mextra_attribs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"rooms_per_hhold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pop_per_hhold\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bedrooms_per_room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcat_one_hot_attribs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_attribs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_attribs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcat_one_hot_attribs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattibutes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'classes_'"
     ]
    }
   ],
   "source": [
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = list(encoder.)\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attibutes), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-118.39     34.12     29.     ... 2184.      960.        8.2816]\n",
      " [-117.86     33.77     39.     ... 1669.      651.        4.6111]\n",
      " [-119.05     34.21     27.     ... 2110.      876.        3.0119]\n",
      " ...\n",
      " [-118.49     34.18     31.     ... 1486.      684.        4.8984]\n",
      " [-117.32     33.99     27.     ... 2400.      836.        4.711 ]\n",
      " [-118.91     36.79     19.     ...  187.       80.        3.7857]]\n",
      "[['<1H OCEAN']\n",
      " ['<1H OCEAN']\n",
      " ['<1H OCEAN']\n",
      " ...\n",
      " ['<1H OCEAN']\n",
      " ['INLAND']\n",
      " ['INLAND']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47429.440582564865"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 세트로 시스템 평가하기\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis = 1)\n",
    "Y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(Y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
