{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 추출하는 함수\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "\n",
    "def load_housing_data(housing_path = HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()\n",
    "#ocean_proximity만 제외하고 모든 특성이 숫자형이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 세트 만들기 \n",
    "# 샘플을 선택해 데이터 셋의 20퍼센트 정도 떼놓으면 된다. \n",
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런을 이용하여 데이터셋을 여러 서브셋으로 만들 수 있다. \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laply/.local/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n",
      "/home/laply/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0    0.350594\n",
       "2.0    0.318859\n",
       "4.0    0.176296\n",
       "5.0    0.114402\n",
       "1.0    0.039850\n",
       "Name: income_cat, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"]/1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_with_id = housing.reset_index()\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝을 위한 데이터 준비 \n",
    "# 예측 변수와 레이블을 분리 \n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 \n",
    "# total_bedrooms 특성에 값이 경우를 고치기\n",
    "# 방법은 3가지 - 해당구역 제거(dropna()), 전체 특성을 삭제(drop()), 어떤 값으로 채우기(fillna())\n",
    "housing.dropna(subset=[\"total_bedrooms\"])     # 1\n",
    "housing.drop(\"total_bedrooms\", axis=1)        # 2    \n",
    "median = housing[\"total_bedrooms\"].median()   # 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 Imputer 를 이용하여 누락된 값을 손쉽게 다루도록 함\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy = \"median\") # 누락된 값을 특성의 중간값으로 변경 \n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # 중간값이 수치형 특성에서만 계산될 수 있기때문에 텍스트 특성을 제외한 복사본 생성 \n",
    "imputer.fit(housing_num) # fit 메서드를 이용하여 훈련데이터에 적용\n",
    "# 어떤 수치형 데이터가 누락될지 모르니까 모든 수치형 특성에 imputer 특성을 적용하는것이 바람직 \n",
    "# imputer는 각 특성의 중간값을 계산하여 그 결과를 객체의 statistics 속성에 저장한다.\n",
    "# imputer.statistic_\n",
    "\n",
    "# 학습된 imputer 객체를 사용해 훈련세트에서 누락된 값을 학습한 중간값으로 바꿀수 있다. \n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = list(housing.index.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17606     <1H OCEAN\n",
       "18632     <1H OCEAN\n",
       "14650    NEAR OCEAN\n",
       "3230         INLAND\n",
       "3555      <1H OCEAN\n",
       "19480        INLAND\n",
       "8879      <1H OCEAN\n",
       "13685        INLAND\n",
       "4937      <1H OCEAN\n",
       "4861      <1H OCEAN\n",
       "Name: ocean_proximity, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텍스트와 범주형 특성 다루기 \n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 0, 2, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대부분의 머신러닝 알고리즘은 숫자형을 다루므로 이 카테고리를 텍스트에서 숫자로 변경한다.\n",
    "# 각 카테고리를 다른 정숫값으로 매핑해주는 판다스의 factorize() 메서드를 사용\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize()\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['<1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카테고리 별 이진특성을 만들어 사용(카테고리가 <1H OCEAN 일때 한 특성은 1 나머지는 0으로 각각 카테고리별로 )\n",
    "# 원-핫 인코딩\n",
    "# 사이킷 런에서 숫자로된 범주형 값을 윈-핫 백터로 바꾸어주는 OneHotEncoder를 제공한다. \n",
    "\n",
    "from future_encoders import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(1, -1)) # 2차원 배열을 1차원 배열로 변경\n",
    "housing_cat_1hot\n",
    "housing_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        if self.categories != 'auto':\n",
    "            for cats in self.categories:\n",
    "                if not np.all(np.sort(cats) == np.array(cats)):\n",
    "                    raise ValueError(\"Unsorted categories are not yet \"\n",
    "                                     \"supported\")\n",
    "\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                if self.handle_unknown == 'error':\n",
    "                    valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                    if not np.all(valid_mask):\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(self.categories[i])\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_temp = check_array(X, dtype=None)\n",
    "        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, str):\n",
    "            X = check_array(X, dtype=np.object)\n",
    "        else:\n",
    "            X = X_temp\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            Xi = X[:, i]\n",
    "            valid_mask = np.in1d(Xi, self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    Xi = Xi.copy()\n",
    "                    Xi[~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        feature_indices = np.cumsum(n_values)\n",
    "\n",
    "        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n",
    "        indptr = X_mask.sum(axis=1).cumsum()\n",
    "        indptr = np.insert(indptr, 0, 0)\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csr_matrix((data, indices, indptr),\n",
    "                                shape=(n_samples, feature_indices[-1]),\n",
    "                                dtype=self.dtype)\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self, 'categories_')\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "\n",
    "        n_samples, _ = X.shape\n",
    "        n_features = len(self.categories_)\n",
    "        n_transformed_features = sum([len(cats) for cats in self.categories_])\n",
    "\n",
    "        # validate shape of passed X\n",
    "        msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n",
    "               \"columns, got {1}.\")\n",
    "        if self.encoding == 'ordinal' and X.shape[1] != n_features:\n",
    "            raise ValueError(msg.format(n_features, X.shape[1]))\n",
    "        elif (self.encoding.startswith('onehot')\n",
    "                and X.shape[1] != n_transformed_features):\n",
    "            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n",
    "\n",
    "        # create resulting array of appropriate dtype\n",
    "        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n",
    "        X_tr = np.empty((n_samples, n_features), dtype=dt)\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            for i in range(n_features):\n",
    "                labels = X[:, i].astype('int64')\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "        else:  # encoding == 'onehot' / 'onehot-dense'\n",
    "            j = 0\n",
    "            found_unknown = {}\n",
    "\n",
    "            for i in range(n_features):\n",
    "                n_categories = len(self.categories_[i])\n",
    "                sub = X[:, j:j + n_categories]\n",
    "\n",
    "                # for sparse X argmax returns 2D matrix, ensure 1D array\n",
    "                labels = np.asarray(_argmax(sub, axis=1)).flatten()\n",
    "                X_tr[:, i] = self.categories_[i][labels]\n",
    "\n",
    "                if self.handle_unknown == 'ignore':\n",
    "                    # ignored unknown categories: we have a row of all zero's\n",
    "                    unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n",
    "                    if unknown.any():\n",
    "                        found_unknown[i] = unknown\n",
    "\n",
    "                j += n_categories\n",
    "\n",
    "            # if ignored are found: potentially need to upcast result to\n",
    "            # insert None values\n",
    "            if found_unknown:\n",
    "                if X_tr.dtype != object:\n",
    "                    X_tr = X_tr.astype(object)\n",
    "\n",
    "                for idx, mask in found_unknown.items():\n",
    "                    X_tr[mask, idx] = None\n",
    "\n",
    "        return X_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 카테고리를 숫자 카테고리로 숫자카테고리를 원-핫 벡터로 바꾸어주는 이 두가지 변환을 \n",
    "# CategoricalEncoder를 이용하여 한번에 할수 있다.\n",
    "\n",
    "cat_encoder = CategoricalEncoder()\n",
    "housing_cat_reshaped = housing_cat.values.reshape(1, -1)\n",
    "cat_encoder = CategoricalEncoder(encoding=\"onehot-dense\")\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)\n",
    "#cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나만의 변환기 \n",
    "# 사이킷런의 기능과 매끄럽게 연동하고 싶을것입니다. 사이킷 런은 덕 타이핑을 지원하므로\n",
    "# fit()-(self를 반환), transfrom(), fit_transform()을 구현한 파이썬 클레스를 만들면 됨\n",
    "# 마지막 메소드(fit_transform())은 TransformerMixin을 상속하면 생성된다.\n",
    "# 또한 BaseEstimator를 상속하면 하이퍼파라미터 튜닝에 필요한 두 메서드를 (get_params(), set_params())를 얻게된다. \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix]/ X[:,household_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,bedrooms_ix]/ X[:,rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else :\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 파이프라인 \n",
    "# 변환단계를 순서대로 실행될 수 있도록 사이킷런의 Pipeline 클레스를 이용하여 수정한다. \n",
    "\n",
    "# 연속된 단계를 나타내는 이름/추정기 쌍의 목록을 입력으로 받는다. \n",
    "# 마지막 단계에는 변환기와 추정기를 모두 사용할 수 있고 나머지는 모두 변환기이어야 한다. 즉 fit_transform() 메서드를 모두 갖고 있어야함\n",
    "# 마지막 추정기가 변환기 StandardScaler 이므로 파이프라인이 데이터에 대해 모든 변환을 순서대로 적용하는 transfrom() 메소드를 갖고있다. \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "using_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-121.89     37.29     38.     ...  339.        2.7042    2.    ]\n",
      " [-121.93     37.05     14.     ...  113.        6.4214    5.    ]\n",
      " [-117.2      32.77     31.     ...  462.        2.8621    2.    ]\n",
      " ...\n",
      " [-116.4      34.09      9.     ...  765.        3.2723    3.    ]\n",
      " [-118.01     33.82     31.     ...  356.        4.0625    3.    ]\n",
      " [-122.45     37.77     52.     ...  639.        3.575     3.    ]]\n",
      "[['<1H OCEAN']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR OCEAN']\n",
      " ...\n",
      " ['INLAND']\n",
      " ['<1H OCEAN']\n",
      " ['NEAR BAY']]\n"
     ]
    }
   ],
   "source": [
    "# 수치형 컬럼을 넘파이 배열로 추출하는대신 판다스의 데이터 프레임을 파이프라인에 직접 주입하면 좋다.\n",
    "# 수치형 컬럼을 넘파이 배열로 추출하는 대싱 판다스의 데이터 프레임을 다룰 수는 없지만 이를 처리하는 변환기를 만들수 있다. \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "class CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sparse_output=False):\n",
    "        self.sparse_output = sparse_output\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        enc = LabelBinarizer(sparse_output=self.sparse_output)\n",
    "        return enc.fit_transform(X)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# 나머지는 버리고 필요한 특성을 선택하명 데이터 프레임을 넘파이 배열로바꾸는 식으로 데이터를 변환한다.\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        print(X[self.attribute_names].values)\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)), # 수치형 특성을 선택한 걸로 파이프라인을 시작\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)), # 범주현 특성을 선택한걸로 파이프라인을 시작 \n",
    "        ('label_binarizer',CustomLabelBinarizer()),\n",
    "    ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion  # 파이프라인을 하나의 파이프라인으로 통합 \n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing) # 전체 파이프라인을 간단하게 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 세트에서 훈련하고 평가 하기\n",
    "# 완전히 작동하는 선형 회귀 모델\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68376.64295459937"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "lin_rmse\n",
    "#과소적합 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결정 트리 회귀 모델 \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "tree_rmse\n",
    "#과대적합 사례"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차검증을 사용한 평가 (k-겹교차 검증) \n",
    "# 훈련세트를 폴드라고 불리는 10개의 서브셋으로 무작위 분할하여 10번 훈련후 평가 매번 다른 폴드를 선택해 평가에 사용 나머지 9개 훈련\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
